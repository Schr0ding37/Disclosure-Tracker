import requests
import re
import time
import json
import random
import os
import psycopg2
import logging
import datetime  # å¿…é ˆå°å…¥æ•´å€‹æ¨¡çµ„ï¼Œ logging è½‰æ›å™¨æ‰æŠ“å¾—åˆ°
from datetime import date
from bs4 import BeautifulSoup
from collections import OrderedDict
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import time

# --- é…ç½® ---
DB_URL = os.getenv("DATABASE_URL", "postgresql://mops:mops123@mops-db:5432/mops")
PROGRESS_FILE = "/app/fetcher/progress.json"
KEYWORDS_FILE = "/app/keywords.txt"
TARGET_YEAR = int(os.getenv("BACKFILL_TARGET_YEAR", 114))

# å®šç¾©ä¸€å€‹å›å‚³å°åŒ—æ™‚é–“çš„å‡½æ•¸ (ä¿®æ­£ç‰ˆ)
def taipei_time(*args):
    # ä½¿ç”¨ timezone æŒ‡å®š UTC+8ï¼Œé¿å…èˆ‡ from datetime import date è¡çª
    tz_plus8 = datetime.timezone(datetime.timedelta(hours=8))
    return datetime.datetime.now(tz_plus8).timetuple()

# é—œéµè¨­å®šï¼šå°‡ logging çš„æ™‚é–“è½‰æ›å™¨æ›¿æ›ç‚ºå°åŒ—æ™‚é–“
logging.Formatter.converter = taipei_time

# è¨­å®šæ—¥èªŒæ ¼å¼
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'  # æŒ‡å®šæ™‚é–“é¡¯ç¤ºæ ¼å¼
)
logger = logging.getLogger("Backfiller")

class MOPSHistoryManager:
    def __init__(self):
        self.session = requests.Session()
        retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
        self.session.mount('https://', HTTPAdapter(max_retries=retries))

        self.keywords = self.load_keywords()

        self.base_headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Content-Type": "application/x-www-form-urlencoded",
            "Referer": "https://mopsov.twse.com.tw/mops/web/t51sb10_q1",
        }

    def load_keywords(self):
        if os.path.exists(KEYWORDS_FILE):
            with open(KEYWORDS_FILE, "r", encoding="utf-8") as f:
                return [line.strip() for line in f if line.strip()]
        return []

    def load_progress(self):
        if os.path.exists(PROGRESS_FILE):
            with open(PROGRESS_FILE, 'r') as f:
                return json.load(f)
        return {"current_year": 115, "current_month": 1, "current_kind_idx": 0, "current_page": 1}

    def save_progress(self, year, month, kind_idx, page):
        with open(PROGRESS_FILE, 'w') as f:
            json.dump({"current_year": year, "current_month": month, "current_kind_idx": kind_idx, "current_page": page}, f)

    def fetch_list(self, year, month, kind, page=1):
        """
        kind: 'L' ç‚ºä¸Šå¸‚, 'O' ç‚ºä¸Šæ«ƒ
        """
        url = "https://mopsov.twse.com.tw/mops/web/ajax_t51sb10"

        payload = {
            "encodeURIComponent": "1", "step": "1", "firstin": "true",
            "TYPEK": "", "Stp": "4", "r1": "1", 
            "KIND": kind, 
            "year": str(year), "month1": str(month), 
            "begin_day": "1", "end_day": "31", 
            "Orderby": "1", "PCount": "100", "pagenum": str(page)
        }

        logger.info(f"  [Wait] éœç½®å¾Œç™¼é€è«‹æ±‚ (æ¨¡æ“¬é–±è®€æ™‚é–“)...")
        time.sleep(random.uniform(10, 15))

        try:
            res = self.session.post(url, data=payload, headers=self.base_headers, timeout=30)
            if "FOR SECURITY REASONS" in res.text: return "BLOCKED"
            res.encoding = 'utf-8'
            return res.text
        except: return "FETCH_FAILED"

    def extract_params(self, html):
        """
        ä½¿ç”¨ BeautifulSoup ç²¾æº–æå–è¡¨æ ¼ä¸­çš„å…¬å‘Šåƒæ•¸
        """
        results = []
        soup = BeautifulSoup(html, 'html.parser')
        
        # 1. æ‰¾åˆ°æ‰€æœ‰è³‡æ–™åˆ— (odd, even)
        rows = soup.find_all('tr', {'class': ['odd', 'even']})
        
        for row in rows:
            tds = row.find_all('td')
            if len(tds) < 6:
                continue
                
            # å–å¾—å…¬å¸åŸºæœ¬è³‡è¨Š (å¾è¡¨æ ¼æ¬„ä½)
            co_code = tds[0].get_text(strip=True)
            co_name = tds[1].get_text(strip=True)
            
            # 2. æ‰¾åˆ°ã€Œè©³ç´°è³‡æ–™ã€æŒ‰éˆ•ï¼Œæå–è£¡é¢çš„ onclick JS ç¨‹å¼ç¢¼
            btn = row.find('input', {'type': 'button', 'value': 'è©³ç´°è³‡æ–™'})
            if btn and btn.get('onclick'):
                onclick_text = btn.get('onclick')
                
                # ä½¿ç”¨ Regex å°ˆé–€æŠ“å– onclick è£¡é¢çš„è®Šæ•¸è³¦å€¼
                # é€™è£¡åŠ å…¥äº† \s* ä¾†è™•ç†å¯èƒ½çš„ç©ºæ ¼æˆ–æ›è¡Œ
                pattern = (
                    r'seq_no\.value\s*=\s*["\'](\d+)["\'];.*?'
                    r'spoke_time\.value\s*=\s*["\'](\d+)["\'];.*?'
                    r'spoke_date\.value\s*=\s*["\'](\d+)["\'];.*?'
                    r'co_id\.value\s*=\s*["\'](\d+)["\'];.*?'
                    r'TYPEK\.value\s*=\s*["\'](\w+)["\']'
                )
                
                m = re.search(pattern, onclick_text, re.DOTALL)
                if m:
                    # çµ„åˆçµæœï¼š(ä»£è™Ÿ, åç¨±, seq_no, èªéŸ³æ™‚é–“, æ—¥æœŸ, co_id, å¸‚å ´é¡å‹)
                    results.append((
                        co_code, 
                        co_name, 
                        m.group(1), 
                        m.group(2), 
                        m.group(3), 
                        m.group(4), 
                        m.group(5)
                    ))
        
        return results

    def fetch_detail(self, p):
        seq_no, spoke_time, spoke_date, co_id, typek = p
        url = "https://mopsov.twse.com.tw/mops/web/ajax_t05st01"
        payload = {
            "encodeURIComponent": "1", "step": "2", "firstin": "1", "off": "1",
            "co_id": co_id, "TYPEK": typek, "spoke_date": spoke_date,
            "spoke_time": spoke_time, "seq_no": seq_no
        }
        
        time.sleep(random.uniform(10, 15)) 
        res = self.session.post(url, data=payload, headers=self.base_headers, timeout=30)
        res.encoding = 'utf-8'
        return res.text

    def parse_detail(self, html):
        soup = BeautifulSoup(html, 'html.parser')
        table = soup.find('table', {'class': 'hasBorder'})
        if not table: return None
        data = {}
        for tr in table.find_all('tr'):
            heads = tr.find_all('td', {'class': 'tblHead'})
            values = tr.find_all('td', {'class': 'odd'})
            if len(heads) == len(values) or (len(heads)==1 and len(values)==1):
                for h, v in zip(heads, values):
                    key = h.get_text(strip=True)
                    pre = v.find('pre')
                    val = pre.get_text().strip() if pre else v.get_text(strip=True)
                    data[key] = val.replace('\xa0', '')
        return data

    def roc_to_ad(self, date_str):
        s = str(date_str).strip()
        # å¦‚æœå·²ç¶“æ˜¯ 8 ä½æ•¸(è¥¿å…ƒ)ï¼Œç›´æ¥æ ¼å¼åŒ–
        if len(s) == 8:
            return f"{s[:4]}-{s[4:6]}-{s[6:]}"
        # å¦‚æœæ˜¯ 7 ä½æ•¸(æ°‘åœ‹)ï¼Œæ‰åšè½‰æ›
        try:
            y = int(s[:-4]) + 1911
            return f"{y:04d}-{s[-4:-2]}-{s[-2:]}"
        except:
            return None

    def normalize_time(self, t):
        s = re.sub(r'[^0-9]', '', str(t)).zfill(6)
        return f"{s[:2]}:{s[2:4]}:{s[4:]}"

    def start_loop(self):
        prog = self.load_progress()
        curr_y, curr_m, curr_k_idx, curr_p = prog["current_year"], prog["current_month"], prog["current_kind_idx"], prog["current_page"]
        markets = [('L', 'ä¸Šå¸‚'), ('O', 'ä¸Šæ«ƒ')]
        
        logger.info(f"ğŸš€ å•Ÿå‹•æ­·å²è£œä»¶ï¼š{curr_y}å¹´{curr_m}æœˆ å¾€å›è£œè‡³ {TARGET_YEAR}å¹´")

        while curr_y >= TARGET_YEAR:
            while curr_m >= 1:
                try:
                    conn = psycopg2.connect(DB_URL)
                    cur = conn.cursor()
                    while curr_k_idx < len(markets):
                        m_kind, m_name = markets[curr_k_idx]
                        logger.info(f"ğŸ“‚ è™•ç†ï¼š{curr_y}å¹´{curr_m}æœˆ | å¸‚å ´ï¼š{m_name}")
                        html = self.fetch_list(curr_y, curr_m, m_kind, curr_p)
                        
                        if html == "BLOCKED":
                            logger.error("ğŸ›‘ åµæ¸¬åˆ°è¡Œç‚ºå°é–ï¼è«‹æš«åœ 30 åˆ†é˜å†è©¦ã€‚")
                            return
                        
                        if html != "NO_DATA" and html != "ERROR":
                            # logger.info(html)
                            matches = self.extract_params(html)
                            logger.info(f"  [+] ç™¼ç¾ {len(matches)} ç­†å…¬å‘Š")
                            for match in matches:
                                # 1. æ‹†è§£ match åƒæ•¸ (é †åºé ˆèˆ‡ extract_params ä¸€è‡´)
                                # match = (co_code, co_name, seq_no, s_time, s_date, co_id_param, typek)
                                co_code, co_name = match[0], match[1].strip()
                                seq_no, s_time, s_date = match[2], match[3], match[4]
                                co_id_param, typek = match[5], match[6]

                                logger.info(f"    - è™•ç† {co_name} ({co_code})")
                                
                                # 2. æŠ“å–è©³ç´°å…§æ–‡
                                detail_html = self.fetch_detail((seq_no, s_time, s_date, co_id_param, typek))
                                d = self.parse_detail(detail_html)

                                if d:
                                    # 3. æ ¼å¼åŒ–æ—¥æœŸèˆ‡æ™‚é–“
                                    # p_date: å¾ '20260115' è½‰ç‚º '2026-01-15'
                                    # p_time: å¾ '85759' è½‰ç‚º '08:57:59'
                                    p_date = self.roc_to_ad(s_date)
                                    p_time = self.normalize_time(s_time)
                                    
                                    # 4. å­˜å…¥è³‡æ–™åº«
                                    cur.execute("""
                                        INSERT INTO disclosures (
                                            market, company_code, company_name, 
                                            publish_date, publish_time, subject, 
                                            content, source_date, fetch_status
                                        )
                                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, TRUE)
                                        ON CONFLICT DO NOTHING RETURNING id
                                    """, (m_name, co_code, co_name, p_date, p_time, d.get("ä¸»æ—¨", ""), d.get("èªªæ˜", ""), date.today()))

                                    res = cur.fetchone()
                                    if res:
                                        # 5. é—œéµå­—æ¯”å°èˆ‡è­¦å ±
                                        disclosure_id = res[0]
                                        full_text = f"{d.get('ä¸»æ—¨','')}{d.get('èªªæ˜','')}"
                                        for kw in self.keywords:
                                            if kw in full_text:
                                                cur.execute("""
                                                    INSERT INTO alerts (disclosure_id, matched_keyword) 
                                                    VALUES (%s, %s) ON CONFLICT DO NOTHING
                                                """, (disclosure_id, kw))

                                    conn.commit() # æ¯è™•ç†å®Œä¸€ç­†å°±æäº¤ï¼Œç¢ºä¿é€²åº¦å­˜æª”

                                # éµå®ˆçˆ¬èŸ²ç¦®ç¯€ï¼Œæ¯ç­†è©³ç´°è³‡æ–™é–“éš”ä¸€ä¸‹
                                time.sleep(random.uniform(2, 3.5))
                        curr_k_idx += 1
                        self.save_progress(curr_y, curr_m, curr_k_idx, curr_p)
                    cur.close(); conn.close()
                    time.sleep(5) # åˆ‡æ›å¸‚å ´æ™‚å¤šä¼‘æ¯ä¸€ä¸‹
                except Exception as e: logger.error(f"âŒ éŒ¯èª¤: {e}"); time.sleep(10)
                curr_m -= 1; curr_k_idx = 0; self.save_progress(curr_y, curr_m, curr_k_idx, curr_p)
                
            curr_y -= 1; curr_m = 12

if __name__ == "__main__":
    MOPSHistoryManager().start_loop()